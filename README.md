# MNIST Digit Classification with the Forward-Forward Algorithm

## Introduction

This repository implements the **Forward-Forward (FF) Algorithm** for classifying digits from the MNIST dataset, as outlined in **[Geoffrey Hinton's paper](https://www.cs.toronto.edu/~hinton/FFA13.pdf)**, *"The Forward-Forward Algorithm: Some Preliminary Investigations."* The FF Algorithm serves as an alternative to backpropagation, where two forward passes are performedâ€”one with positive data (real MNIST digits) and another with negative data (corrupted or generated inputs). The network adjusts weights to maximize the *"goodness"* of positive data and minimize the *"goodness"* of negative data.

The repository is divided into two parts:
1. **Supervised Learning**: Implementing the FF Algorithm with labeled data.
2. **Unsupervised Learning**: Applying the FF Algorithm without relying on labeled data.

### Objectives

1. **Forward-Forward Algorithm Implementation**: The main goal is to implement and test the FF learning algorithm on the MNIST dataset.
2. **Performance Enhancement**: Several techniques are employed to enhance the model's performance.
3. **Preprocessing & Model Design**: Preprocessing techniques and model structures are explored to optimize the classification process.

### Key Highlights

- **Two Forward Passes**: The FF Algorithm performs forward passes with both positive and negative data to adjust weights, maximizing the network's *"goodness"* for positive data while minimizing it for negative data.
- **Local Layer Optimization**: Each hidden layer focuses on maximizing its local goodness, which streamlines the learning process.
- **Continuous Improvement**: Various techniques are applied to improve accuracy, following the guidance from the original paper and experimental results.

---

## Supervised Learning: Forward-Forward Algorithm

In the **Supervised Learning** implementation, the Forward-Forward Algorithm is applied to the MNIST dataset with labeled data. The algorithm adjusts the network's weights based on the goodness scores for both positive and negative samples.

### Key Features

- **Custom FF Layers**: The architecture includes custom FF layers designed to iteratively update weights based on the forward pass results.
- **Threshold-Based Loss**: A custom loss function pushes the squared means of positive samples toward a threshold and negative samples away from it, ensuring effective classification.
- **Training & Evaluation**: The model is trained for 30 epochs using the MNIST dataset, and both training and testing accuracies are evaluated to monitor performance.

### Results

The supervised model is tested over multiple epochs, and error rates are plotted to provide insights into the learning progression. 
The results indicate strong generalization with minimal overfitting.

---

## Unsupervised Learning: Forward-Forward Algorithm

### Overview

In the **Unsupervised Learning** section of the project, the Forward-Forward Algorithm is applied without labeled data. The unsupervised version focuses on generating negative data by creating hybrid images through the use of masks. These masks blur regions of digit images and combine them with other digits, following the procedure described in Geoffrey Hinton's paper.

### Key Features

- **Hybrid Image Generation**: Negative examples are generated by combining two MNIST digit images with a mask that ensures long-range correlations remain distinct while short-range correlations are similar.
- **Custom FF Layers**: Similar to the supervised version, this model uses custom FF layers, which normalize inputs and update weights through multiple forward passes.
- **Training & Evaluation**: The model is trained over multiple epochs, and the accuracy of the unsupervised approach is evaluated on both the training and test datasets, yielding strong generalization despite the absence of labeled data.

### Results

- After 7 epochs of training, the unsupervised model achieves approximately 0.9 accuracy on the training dataset and around 0.88 accuracy on the test dataset.
- The loss over epochs is plotted to monitor the learning progress, and the model demonstrates competitive performance without relying on labels.

---

For more details, refer to the accompanying notebooks and scripts in this repository. 
This project showcases the versatility of the Forward-Forward Algorithm across both supervised and unsupervised learning paradigms, highlighting its potential for further research and development.
